# Crawl4AI éƒ¨ç½²æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚
- Python 3.9+
- 8GB+ RAM (æ¨è)
- ç½‘ç»œè¿æ¥

### 2. æœ¬åœ°å®‰è£…

#### æ–¹å¼ä¸€ï¼šä»æºç å®‰è£…ï¼ˆæ¨èå¼€å‘ï¼‰
```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -e .

# è®¾ç½®æµè§ˆå™¨
crawl4ai-setup

# éªŒè¯å®‰è£…
crawl4ai-doctor
```

#### æ–¹å¼äºŒï¼špipå®‰è£…ï¼ˆæ¨èç”Ÿäº§ï¼‰
```bash
# å®‰è£…åŒ…
pip install crawl4ai

# è®¾ç½®æµè§ˆå™¨
crawl4ai-setup

# éªŒè¯å®‰è£…
crawl4ai-doctor
```

### 3. Dockeréƒ¨ç½²

#### å¿«é€Ÿå¯åŠ¨
```bash
# æ‹‰å–å¹¶è¿è¡Œæœ€æ–°ç‰ˆæœ¬
docker pull unclecode/crawl4ai:0.7.0
docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.7.0

# æµ‹è¯•æœåŠ¡
curl http://localhost:11235/health
```

#### Docker Composeéƒ¨ç½²
```yaml
# docker-compose.yml
version: '3.8'
services:
  crawl4ai:
    image: unclecode/crawl4ai:0.7.0
    ports:
      - "11235:11235"
    shm_size: 1g
    environment:
      - CRAWL4AI_LOG_LEVEL=INFO
      - CRAWL4AI_MAX_CONCURRENT=10
    volumes:
      - ./data:/app/data
    restart: unless-stopped
```

```bash
# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f crawl4ai
```

## ğŸ”§ é…ç½®é€‰é¡¹

### 1. æµè§ˆå™¨é…ç½®
```python
from crawl4ai import BrowserConfig

browser_config = BrowserConfig(
    headless=True,              # æ— å¤´æ¨¡å¼
    browser_type="chromium",    # æµè§ˆå™¨ç±»å‹
    viewport_width=1920,        # è§†å£å®½åº¦
    viewport_height=1080,       # è§†å£é«˜åº¦
    user_agent="custom-agent",  # è‡ªå®šä¹‰UA
    proxy="http://proxy:8080",  # ä»£ç†è®¾ç½®
    extra_args=[               # é¢å¤–å‚æ•°
        "--disable-blink-features=AutomationControlled",
        "--disable-web-security"
    ]
)
```

### 2. çˆ¬å–é…ç½®
```python
from crawl4ai import CrawlerRunConfig, CacheMode

run_config = CrawlerRunConfig(
    cache_mode=CacheMode.ENABLED,     # ç¼“å­˜æ¨¡å¼
    word_count_threshold=50,          # æœ€å°è¯æ•°
    wait_for="networkidle",           # ç­‰å¾…æ¡ä»¶
    page_timeout=30000,               # é¡µé¢è¶…æ—¶
    js_code=["console.log('hello')"], # JSä»£ç 
    css_selector="main",              # CSSé€‰æ‹©å™¨
    screenshot=True,                  # æˆªå›¾
    pdf=True                          # ç”ŸæˆPDF
)
```

### 3. ç¯å¢ƒå˜é‡é…ç½®
```bash
# .env æ–‡ä»¶
CRAWL4AI_LOG_LEVEL=INFO
CRAWL4AI_MAX_CONCURRENT=10
CRAWL4AI_CACHE_DIR=/tmp/crawl4ai
CRAWL4AI_DB_PATH=/data/crawl4ai.db
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-claude-key
```

## ğŸ—ï¸ ç”Ÿäº§éƒ¨ç½²

### 1. æ€§èƒ½ä¼˜åŒ–

#### å¹¶å‘æ§åˆ¶
```python
# è®¾ç½®åˆç†çš„å¹¶å‘æ•°
browser_config = BrowserConfig(
    max_concurrent_pages=5,  # æ¯ä¸ªæµè§ˆå™¨æœ€å¤§é¡µé¢æ•°
    browser_pool_size=3      # æµè§ˆå™¨æ± å¤§å°
)

# æ‰¹é‡å¤„ç†
urls = ["url1", "url2", "url3", ...]
results = await crawler.arun_many(urls, config=run_config)
```

#### å†…å­˜ç®¡ç†
```python
# å®šæœŸæ¸…ç†ç¼“å­˜
from crawl4ai.cache_context import CacheContext

cache = CacheContext()
await cache.clear_expired()  # æ¸…ç†è¿‡æœŸç¼“å­˜
await cache.optimize()       # ä¼˜åŒ–å­˜å‚¨
```

### 2. ç›‘æ§å’Œæ—¥å¿—

#### æ—¥å¿—é…ç½®
```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawl4ai.log'),
        logging.StreamHandler()
    ]
)
```

#### æ€§èƒ½ç›‘æ§
```python
from crawl4ai.utils import MemoryMonitor

monitor = MemoryMonitor()
monitor.start_monitoring()

# æ‰§è¡Œçˆ¬å–ä»»åŠ¡
results = await crawler.arun_many(urls)

# è·å–æ€§èƒ½æŠ¥å‘Š
report = monitor.get_report()
print(f"å³°å€¼å†…å­˜: {report['peak_mb']:.1f} MB")
print(f"æ•ˆç‡: {report['efficiency']:.1f}%")
```

### 3. é«˜å¯ç”¨éƒ¨ç½²

#### è´Ÿè½½å‡è¡¡
```nginx
# nginx.conf
upstream crawl4ai_backend {
    server 127.0.0.1:11235;
    server 127.0.0.1:11236;
    server 127.0.0.1:11237;
}

server {
    listen 80;
    location / {
        proxy_pass http://crawl4ai_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

#### å®¹å™¨ç¼–æ’ (Kubernetes)
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crawl4ai
spec:
  replicas: 3
  selector:
    matchLabels:
      app: crawl4ai
  template:
    metadata:
      labels:
        app: crawl4ai
    spec:
      containers:
      - name: crawl4ai
        image: unclecode/crawl4ai:0.7.0
        ports:
        - containerPort: 11235
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        env:
        - name: CRAWL4AI_MAX_CONCURRENT
          value: "5"
---
apiVersion: v1
kind: Service
metadata:
  name: crawl4ai-service
spec:
  selector:
    app: crawl4ai
  ports:
  - port: 80
    targetPort: 11235
  type: LoadBalancer
```

## ğŸ”’ å®‰å…¨é…ç½®

### 1. APIè®¤è¯
```python
# å¯ç”¨JWTè®¤è¯
from crawl4ai.auth import JWTAuth

auth = JWTAuth(secret_key="your-secret-key")
app.add_middleware(auth)
```

### 2. ç½‘ç»œå®‰å…¨
```bash
# é˜²ç«å¢™é…ç½®
sudo ufw allow 11235/tcp
sudo ufw enable

# é™åˆ¶è®¿é—®IP
iptables -A INPUT -p tcp --dport 11235 -s 192.168.1.0/24 -j ACCEPT
iptables -A INPUT -p tcp --dport 11235 -j DROP
```

### 3. æ•°æ®ä¿æŠ¤
```python
# æ•æ„Ÿæ•°æ®è„±æ•
def sanitize_content(content):
    import re
    # ç§»é™¤é‚®ç®±
    content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', content)
    # ç§»é™¤ç”µè¯
    content = re.sub(r'\b\d{3}-\d{3}-\d{4}\b', '[PHONE]', content)
    return content
```

## ğŸ“Š ç›‘æ§å’Œç»´æŠ¤

### 1. å¥åº·æ£€æŸ¥
```python
# è‡ªå®šä¹‰å¥åº·æ£€æŸ¥
async def health_check():
    try:
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        await db.execute("SELECT 1")
        
        # æµ‹è¯•æµè§ˆå™¨
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun("https://httpbin.org/status/200")
            
        return {"status": "healthy", "timestamp": time.time()}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

### 2. æ€§èƒ½è°ƒä¼˜
```bash
# ç³»ç»Ÿä¼˜åŒ–
echo 'vm.max_map_count=262144' >> /etc/sysctl.conf
echo 'fs.file-max=65536' >> /etc/sysctl.conf
sysctl -p

# Chromeä¼˜åŒ–
export CHROME_FLAGS="--no-sandbox --disable-dev-shm-usage --disable-gpu"
```

### 3. å¤‡ä»½ç­–ç•¥
```bash
#!/bin/bash
# backup.sh - æ•°æ®å¤‡ä»½è„šæœ¬

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/crawl4ai"

# å¤‡ä»½æ•°æ®åº“
cp ~/.crawl4ai/crawl4ai.db $BACKUP_DIR/crawl4ai_$DATE.db

# å¤‡ä»½ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
tar -czf $BACKUP_DIR/cache_$DATE.tar.gz ~/.crawl4ai/cache/

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™7å¤©ï¼‰
find $BACKUP_DIR -name "*.db" -mtime +7 -delete
find $BACKUP_DIR -name "*.tar.gz" -mtime +7 -delete
```

## ğŸš¨ æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

#### æµè§ˆå™¨å¯åŠ¨å¤±è´¥
```bash
# æ£€æŸ¥ä¾èµ–
playwright install --with-deps chromium

# æƒé™é—®é¢˜
chmod +x ~/.cache/ms-playwright/chromium-*/chrome-linux/chrome
```

#### å†…å­˜ä¸è¶³
```python
# å‡å°‘å¹¶å‘æ•°
browser_config = BrowserConfig(max_concurrent_pages=2)

# å¯ç”¨å†…å­˜ç›‘æ§
from crawl4ai.utils import MemoryMonitor
monitor = MemoryMonitor(threshold_mb=4000)
```

#### ç½‘ç»œè¶…æ—¶
```python
# å¢åŠ è¶…æ—¶æ—¶é—´
run_config = CrawlerRunConfig(
    page_timeout=60000,  # 60ç§’
    navigation_timeout=30000  # 30ç§’
)
```

### 2. è°ƒè¯•æŠ€å·§
```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.getLogger('crawl4ai').setLevel(logging.DEBUG)

# ä¿å­˜è°ƒè¯•ä¿¡æ¯
run_config = CrawlerRunConfig(
    screenshot=True,  # ä¿å­˜æˆªå›¾
    pdf=True,         # ä¿å­˜PDF
    verbose=True      # è¯¦ç»†è¾“å‡º
)
```

## ğŸ“ˆ æ‰©å±•å’Œå®šåˆ¶

### 1. è‡ªå®šä¹‰æå–ç­–ç•¥
```python
from crawl4ai.extraction_strategy import ExtractionStrategy

class CustomExtractionStrategy(ExtractionStrategy):
    async def extract(self, url, html, **kwargs):
        # è‡ªå®šä¹‰æå–é€»è¾‘
        return {"custom_data": "extracted"}
```

### 2. æ’ä»¶å¼€å‘
```python
from crawl4ai.hooks import CrawlHook

class CustomHook(CrawlHook):
    async def before_crawl(self, url, config):
        print(f"å¼€å§‹çˆ¬å–: {url}")
    
    async def after_crawl(self, url, result):
        print(f"å®Œæˆçˆ¬å–: {url}")
```

è¿™ä¸ªéƒ¨ç½²æŒ‡å—æ¶µç›–äº†ä»å¼€å‘ç¯å¢ƒåˆ°ç”Ÿäº§ç¯å¢ƒçš„å®Œæ•´éƒ¨ç½²æµç¨‹ï¼ŒåŒ…æ‹¬æ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨é…ç½®å’Œæ•…éšœæ’é™¤ç­‰å…³é”®æ–¹é¢ã€‚