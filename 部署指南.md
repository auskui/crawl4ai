# Crawl4AI 部署指南

## 🚀 快速开始

### 1. 环境要求
- Python 3.9+
- 8GB+ RAM (推荐)
- 网络连接

### 2. 本地安装

#### 方式一：从源码安装（推荐开发）
```bash
# 克隆项目
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai

# 创建虚拟环境
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# 或 venv\Scripts\activate  # Windows

# 安装依赖
pip install -e .

# 设置浏览器
crawl4ai-setup

# 验证安装
crawl4ai-doctor
```

#### 方式二：pip安装（推荐生产）
```bash
# 安装包
pip install crawl4ai

# 设置浏览器
crawl4ai-setup

# 验证安装
crawl4ai-doctor
```

### 3. Docker部署

#### 快速启动
```bash
# 拉取并运行最新版本
docker pull unclecode/crawl4ai:0.7.0
docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.7.0

# 测试服务
curl http://localhost:11235/health
```

#### Docker Compose部署
```yaml
# docker-compose.yml
version: '3.8'
services:
  crawl4ai:
    image: unclecode/crawl4ai:0.7.0
    ports:
      - "11235:11235"
    shm_size: 1g
    environment:
      - CRAWL4AI_LOG_LEVEL=INFO
      - CRAWL4AI_MAX_CONCURRENT=10
    volumes:
      - ./data:/app/data
    restart: unless-stopped
```

```bash
# 启动服务
docker-compose up -d

# 查看日志
docker-compose logs -f crawl4ai
```

## 🔧 配置选项

### 1. 浏览器配置
```python
from crawl4ai import BrowserConfig

browser_config = BrowserConfig(
    headless=True,              # 无头模式
    browser_type="chromium",    # 浏览器类型
    viewport_width=1920,        # 视口宽度
    viewport_height=1080,       # 视口高度
    user_agent="custom-agent",  # 自定义UA
    proxy="http://proxy:8080",  # 代理设置
    extra_args=[               # 额外参数
        "--disable-blink-features=AutomationControlled",
        "--disable-web-security"
    ]
)
```

### 2. 爬取配置
```python
from crawl4ai import CrawlerRunConfig, CacheMode

run_config = CrawlerRunConfig(
    cache_mode=CacheMode.ENABLED,     # 缓存模式
    word_count_threshold=50,          # 最小词数
    wait_for="networkidle",           # 等待条件
    page_timeout=30000,               # 页面超时
    js_code=["console.log('hello')"], # JS代码
    css_selector="main",              # CSS选择器
    screenshot=True,                  # 截图
    pdf=True                          # 生成PDF
)
```

### 3. 环境变量配置
```bash
# .env 文件
CRAWL4AI_LOG_LEVEL=INFO
CRAWL4AI_MAX_CONCURRENT=10
CRAWL4AI_CACHE_DIR=/tmp/crawl4ai
CRAWL4AI_DB_PATH=/data/crawl4ai.db
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-claude-key
```

## 🏗️ 生产部署

### 1. 性能优化

#### 并发控制
```python
# 设置合理的并发数
browser_config = BrowserConfig(
    max_concurrent_pages=5,  # 每个浏览器最大页面数
    browser_pool_size=3      # 浏览器池大小
)

# 批量处理
urls = ["url1", "url2", "url3", ...]
results = await crawler.arun_many(urls, config=run_config)
```

#### 内存管理
```python
# 定期清理缓存
from crawl4ai.cache_context import CacheContext

cache = CacheContext()
await cache.clear_expired()  # 清理过期缓存
await cache.optimize()       # 优化存储
```

### 2. 监控和日志

#### 日志配置
```python
import logging

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawl4ai.log'),
        logging.StreamHandler()
    ]
)
```

#### 性能监控
```python
from crawl4ai.utils import MemoryMonitor

monitor = MemoryMonitor()
monitor.start_monitoring()

# 执行爬取任务
results = await crawler.arun_many(urls)

# 获取性能报告
report = monitor.get_report()
print(f"峰值内存: {report['peak_mb']:.1f} MB")
print(f"效率: {report['efficiency']:.1f}%")
```

### 3. 高可用部署

#### 负载均衡
```nginx
# nginx.conf
upstream crawl4ai_backend {
    server 127.0.0.1:11235;
    server 127.0.0.1:11236;
    server 127.0.0.1:11237;
}

server {
    listen 80;
    location / {
        proxy_pass http://crawl4ai_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

#### 容器编排 (Kubernetes)
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crawl4ai
spec:
  replicas: 3
  selector:
    matchLabels:
      app: crawl4ai
  template:
    metadata:
      labels:
        app: crawl4ai
    spec:
      containers:
      - name: crawl4ai
        image: unclecode/crawl4ai:0.7.0
        ports:
        - containerPort: 11235
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        env:
        - name: CRAWL4AI_MAX_CONCURRENT
          value: "5"
---
apiVersion: v1
kind: Service
metadata:
  name: crawl4ai-service
spec:
  selector:
    app: crawl4ai
  ports:
  - port: 80
    targetPort: 11235
  type: LoadBalancer
```

## 🔒 安全配置

### 1. API认证
```python
# 启用JWT认证
from crawl4ai.auth import JWTAuth

auth = JWTAuth(secret_key="your-secret-key")
app.add_middleware(auth)
```

### 2. 网络安全
```bash
# 防火墙配置
sudo ufw allow 11235/tcp
sudo ufw enable

# 限制访问IP
iptables -A INPUT -p tcp --dport 11235 -s 192.168.1.0/24 -j ACCEPT
iptables -A INPUT -p tcp --dport 11235 -j DROP
```

### 3. 数据保护
```python
# 敏感数据脱敏
def sanitize_content(content):
    import re
    # 移除邮箱
    content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', content)
    # 移除电话
    content = re.sub(r'\b\d{3}-\d{3}-\d{4}\b', '[PHONE]', content)
    return content
```

## 📊 监控和维护

### 1. 健康检查
```python
# 自定义健康检查
async def health_check():
    try:
        # 测试数据库连接
        await db.execute("SELECT 1")
        
        # 测试浏览器
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun("https://httpbin.org/status/200")
            
        return {"status": "healthy", "timestamp": time.time()}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

### 2. 性能调优
```bash
# 系统优化
echo 'vm.max_map_count=262144' >> /etc/sysctl.conf
echo 'fs.file-max=65536' >> /etc/sysctl.conf
sysctl -p

# Chrome优化
export CHROME_FLAGS="--no-sandbox --disable-dev-shm-usage --disable-gpu"
```

### 3. 备份策略
```bash
#!/bin/bash
# backup.sh - 数据备份脚本

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/crawl4ai"

# 备份数据库
cp ~/.crawl4ai/crawl4ai.db $BACKUP_DIR/crawl4ai_$DATE.db

# 备份缓存（可选）
tar -czf $BACKUP_DIR/cache_$DATE.tar.gz ~/.crawl4ai/cache/

# 清理旧备份（保留7天）
find $BACKUP_DIR -name "*.db" -mtime +7 -delete
find $BACKUP_DIR -name "*.tar.gz" -mtime +7 -delete
```

## 🚨 故障排除

### 1. 常见问题

#### 浏览器启动失败
```bash
# 检查依赖
playwright install --with-deps chromium

# 权限问题
chmod +x ~/.cache/ms-playwright/chromium-*/chrome-linux/chrome
```

#### 内存不足
```python
# 减少并发数
browser_config = BrowserConfig(max_concurrent_pages=2)

# 启用内存监控
from crawl4ai.utils import MemoryMonitor
monitor = MemoryMonitor(threshold_mb=4000)
```

#### 网络超时
```python
# 增加超时时间
run_config = CrawlerRunConfig(
    page_timeout=60000,  # 60秒
    navigation_timeout=30000  # 30秒
)
```

### 2. 调试技巧
```python
# 启用详细日志
import logging
logging.getLogger('crawl4ai').setLevel(logging.DEBUG)

# 保存调试信息
run_config = CrawlerRunConfig(
    screenshot=True,  # 保存截图
    pdf=True,         # 保存PDF
    verbose=True      # 详细输出
)
```

## 📈 扩展和定制

### 1. 自定义提取策略
```python
from crawl4ai.extraction_strategy import ExtractionStrategy

class CustomExtractionStrategy(ExtractionStrategy):
    async def extract(self, url, html, **kwargs):
        # 自定义提取逻辑
        return {"custom_data": "extracted"}
```

### 2. 插件开发
```python
from crawl4ai.hooks import CrawlHook

class CustomHook(CrawlHook):
    async def before_crawl(self, url, config):
        print(f"开始爬取: {url}")
    
    async def after_crawl(self, url, result):
        print(f"完成爬取: {url}")
```

这个部署指南涵盖了从开发环境到生产环境的完整部署流程，包括性能优化、安全配置和故障排除等关键方面。